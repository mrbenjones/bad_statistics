# Introduction to statistical significance.
## Usually comes down to : what is the chance that you'd get the

 result if there is there is no relationship?
We pick a threshold, and we reject the null hypothesis (leaving the
idea that there is something going on), if the probability that the
relationship appears in the numbers with no causual relationship is
sufficiently low (5% is common).

## One key example : medicine effectiveness.
### Example: test how many colds are gone after 20 patients get a cold
medicine.
### Problem: there could be enough variation that you get the effect
you want just because you're lucky.

| Fundamental Frequentist Question |
|----------------------------------|
| If I were full of shit, how      |
| likely would it be that I would  |
| STILL look right?                |

### Coarse estimate of our case: flipping coins
Each patient has some chance of ge3tting better or worse.
If 20 patients get better under our protocol, and the result is 50-50
with no treatment, then the chance this happened by chance is 1 in
2^20 (very low).

### What they call this: *p* value.
Probability of no difference getting the observed result.

### Thinking: *p* value measures **level of surprise**.
If you assume nothing special, the p value tells you how surprised you
would be.

### Tiny musing on the *p* value.
Can we track how harmful using a particular *p* value would be?

### Some hidden assumptions.
Applying the *p* value like this means that we are assuming there is
no difference between the groups, and seeing if we are surprised by
the result under these assumptions.

| *p* values and spackle |
|------------------------|
| Because *p* values just|
| measure surprise, you  |
| can apply them with    |
| math-phrased hypothesis|
| we want to disprove.   |


## Limitations of *p* values.
Level of surprise does NOT measure size of effect.  And this
is a key problem.  You get probability of one answer to a yes
or no question.

If you get a huge study, you will demonstrate near certainty of a
tiny effect.

## Psychic Statistics.

### *p* value definition in other words.
The chance that you would bet a result equal to the current result *or
more extreme* if there was no relationship.

You have to reason about result that didn't occur.  Are the more
extreme cases the whole space?

### Detailed example: Direcdt quote.

> Suppose I ask you a series of 12 true-or-false questions about
>  statistical inference, and you correctly answer 9 of them.  I want
>  to test the hypothesis that you answered the questions by guessing
>  randomly.  To do this, I need to compute the chances of you getting
>  * at least* 9 answers right by simply picking true or false
>  randomly for each question.  Assuming you pick true and false with
>  equal probability, I compute *p* = 0.073. (binomial) Since p >
>  0.05, it's plausible that you guessed randomly.  If you did so,
>  you'd get this many **or more** right 7.3% of the time.
>
> But perhaps it was not my original plan to ask you only 12
>  questions.  Maybe I had a computer that generated a limitless
>  supply of questions and simply asked questions until you got 3
>  wrong.  Now I have to compute the probability of you getting 3
>  questions wrong after being asked 15 or 20 or 47 of them.  I even
>  have to include the remote possibility that you made it to 175231
>  questions before getting 3 of them wrong. Doing this, you find
>  p=0.033.  Since p< 0.05, I conclude that random guessing would be
>  unlikely to produce this result. 